{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc86cb3-6a11-4ba7-81c7-a771be0762c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from soccer_env import soccerenv\n",
    "\n",
    "env = soccerenv(render_mode='human')  # set \"human\" to visualize\n",
    "num_episodes = 5\n",
    "\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    obs, infos = env.reset(options={\"use_full_random_positions\": True})\n",
    "    final_score = {\"blue\": 0, \"red\": 0}\n",
    "    episode_returns = {agent: 0.0 for agent in env.possible_agents}\n",
    "\n",
    "    while env.agents:  # episode runs until env clears agents\n",
    "        actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "        obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        # Pick one agent's info (all agents share identical info each step)\n",
    "        any_agent = next(iter(infos))\n",
    "        step_info = infos[any_agent]\n",
    "        if \"score\" in step_info:\n",
    "            final_score = step_info[\"score\"]\n",
    "        for agent, r in rewards.items():\n",
    "            episode_returns[agent] += float(r)\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Episode {ep} final score: blue={final_score.get('blue',0)}, red={final_score.get('red',0)}\")\n",
    "    print(f\"Episode {ep} returns:\", episode_returns)\n",
    "    print(\"--------------\")\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c3f6c-5371-45a8-bdc2-504ad1d3f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae6454-68b7-44e2-9d43-96df3692f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soccer_env import make_env\n",
    "\n",
    "dummy_env = make_env()\n",
    "\n",
    "env = soccerenv(render_mode=None)\n",
    "for agent in env.agents:\n",
    "    obs_space = env.observation_space(agent)\n",
    "    print(f\"Observation space for {agent}: {obs_space}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff5ae2-a9a3-4482-b7ae-791fab9791c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter as writer\n",
    "from soccer_env import make_env\n",
    "\n",
    "# Imports for PettingZoo and SuperSuit\n",
    "from soccer_env import soccerenv # Make sure soccer_env.py is in the same directory\n",
    "from marl_vecenv import SyncMultiAgentVecEnv\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    \"\"\"Initializes network layers orthogonally.\"\"\"\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, rpo_alpha,device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Check if the env object is the vectorized wrapper or a raw env\n",
    "        if hasattr(envs, 'single_observation_space'):\n",
    "            # It's the vectorized wrapper from training\n",
    "            obs_space = envs.single_observation_space\n",
    "            act_space = envs.single_action_space\n",
    "        else:\n",
    "            # It's a raw PettingZoo env from evaluation\n",
    "            any_agent = envs.possible_agents[0]\n",
    "            obs_space = envs.observation_space(any_agent)\n",
    "            act_space = envs.action_space(any_agent)\n",
    "\n",
    "        obs_shape = np.prod(obs_space.shape)\n",
    "        act_shape = np.prod(act_space.shape)\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(obs_shape, 512)),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(obs_shape, 512)),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(128, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, act_shape), std=0.01),\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, act_shape))\n",
    "        self.rpo_alpha = rpo_alpha\n",
    "        self.device=device\n",
    "\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_mean = self.actor_mean(x)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        else:  # new to RPO\n",
    "            # sample again to add stochasticity to the policy\n",
    "            z = torch.FloatTensor(action_mean.shape).uniform_(-self.rpo_alpha, self.rpo_alpha).to(self.device)\n",
    "            action_mean = action_mean + z\n",
    "            probs = Normal(action_mean, action_std)\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)\n",
    "\n",
    "    def get_deterministic_action(self, x):\n",
    "        action_mean = self.actor_mean(x)\n",
    "        return action_mean # Return the mean directly\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # General parameters\n",
    "    exp_name: str = \"ppo_pettingzoo_soccer\"\n",
    "    seed: int = 19\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = True\n",
    "    \n",
    "    # Algorithm specific arguments\n",
    "    total_timesteps: int = int(20e6)\n",
    "    learning_rate: float = 2e-4\n",
    "    num_steps: int = 4096 # The number of steps to run in each environment per policy rollout\n",
    "    anneal_lr: bool = True\n",
    "    gamma: float = 0.995\n",
    "    gae_lambda: float = 0.95\n",
    "    num_minibatches: int = 16\n",
    "    update_epochs: int = 8\n",
    "    norm_adv: bool = True\n",
    "    clip_coef: float = 0.2\n",
    "    clip_vloss: bool = True\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.7\n",
    "    max_grad_norm: float = 0.5\n",
    "    target_kl: float = None\n",
    "    num_envs: int = 8\n",
    "    rpo_alpha: float = 0.0\n",
    "\n",
    "    # These are computed in runtime\n",
    "    batch_size: int = 0\n",
    "    minibatch_size: int = 0\n",
    "    num_iterations: int = 0\n",
    "\n",
    "    save_model: bool = True\n",
    "    env_id: str = 'SoccerTwos'\n",
    "\n",
    "# Instantiate the config\n",
    "config = Config()\n",
    "\n",
    "\n",
    "# Create a list of environment constructor functions\n",
    "env_fns = [make_env for _ in range(config.num_envs)]\n",
    "\n",
    "# Use the new synchronous wrapper\n",
    "envs = SyncMultiAgentVecEnv(env_fns)\n",
    "# --- END REPLACEMENT CODE ---\n",
    "\n",
    "# Your PPO agent initialization will now work correctly\n",
    "# The wrapper provides the `single_observation_space` attribute\n",
    "#agent = Agent(envs)\n",
    "\n",
    "# The shape of observations will be (num_envs, num_agents, obs_dim)\n",
    "# which is perfect for batch processing.\n",
    "observations = envs.reset(seed=config.seed)\n",
    "print(\"Shape of observations:\", observations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db0847-df50-4c14-8592-bca3152d172c",
   "metadata": {},
   "outputs": [],
   "source": [
    " envs.single_observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1735fb-b68f-478f-adad-9dc960cfee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeanStd:\n",
    "    \"\"\"\n",
    "    Calculates the running mean and standard deviation of a data stream\n",
    "    using Welford's online algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        \n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "\n",
    "        # Update the mean\n",
    "        self.mean += delta * batch_count / tot_count\n",
    "        \n",
    "        # Update the variance\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count\n",
    "        self.var = M2 / tot_count\n",
    "        \n",
    "        # Update the count\n",
    "        self.count = tot_count\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32ce45-fcc6-4c5d-b988-5c8614de085b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Assume these are defined elsewhere:\n",
    "# from cleanrl_utils.experimental import Agent\n",
    "# writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(args, envs, device='cpu', run_name=\"run5\",saved_normalizer=None,model_path=None):\n",
    "    # --- SETUP: Define agent configuration and batch sizes ---\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "\n",
    "    trainable_agent_ids = [\"agent_0\", \"agent_1\"]\n",
    "    possible_agents = envs.possible_agents\n",
    "    trainable_agent_indices = [possible_agents.index(id) for id in trainable_agent_ids]\n",
    "    random_agent_indices = [i for i in range(len(possible_agents)) if i not in trainable_agent_indices]\n",
    "    num_trainable_agents = len(trainable_agent_ids)\n",
    "\n",
    "    normalizer_path = f\"runs/{run_name}/latest_normalizer_stats.npz\"\n",
    "\n",
    "    args.batch_size = int(args.num_envs * args.num_steps * num_trainable_agents)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    args.num_iterations = args.total_timesteps // (args.num_envs * args.num_steps)\n",
    "\n",
    "    agent = Agent(envs,args.rpo_alpha).to(device)\n",
    "    if model_path:\n",
    "        agent.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "    # --- STORAGE: Sized for trainable agents only ---\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs, num_trainable_agents) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs, num_trainable_agents) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs, num_trainable_agents)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs, num_trainable_agents)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs, num_trainable_agents)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs, num_trainable_agents)).to(device)\n",
    "    trunc_all = np.full((args.num_envs,4),False)\n",
    "\n",
    "    # --- INITIALIZATION ---\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs_all_agents = torch.Tensor(envs.reset(seed=args.seed)).to(device)\n",
    "    next_obs = next_obs_all_agents[:, trainable_agent_indices]\n",
    "    next_done = torch.zeros(args.num_envs, num_trainable_agents).to(device)\n",
    "\n",
    "    obs_normalizer = RunningMeanStd(shape=envs.single_observation_space.shape)\n",
    "    if saved_normalizer:\n",
    "        stats = np.load(saved_normalizer)\n",
    "        obs.mean = stats['mean']\n",
    "        obs.std = np.sqrt(stats['var'])\n",
    "\n",
    "    game_rewards=torch.zeros_like(rewards[0])\n",
    "    for iteration in range(1, args.num_iterations + 1):\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = max(lrnow,3e-5)\n",
    "\n",
    "        # --- ROLLOUT PHASE ---\n",
    "        score = [{\"blue\": 0, \"red\": 0} for i in range(args.num_envs)]\n",
    "        games=0\n",
    "        rw = torch.zeros(num_trainable_agents)\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # Action selection and reshaping\n",
    "            with torch.no_grad():\n",
    "                agent_input = next_obs.reshape(-1, *envs.single_observation_space.shape)\n",
    "                normalized_input = np.clip((agent_input.cpu().numpy() - obs_normalizer.mean) / (obs_normalizer.std + 1e-8), -10, 10)\n",
    "                action, logprob, _, value = agent.get_action_and_value(torch.tensor(normalized_input,dtype=torch.float))\n",
    "                values[step] = value.reshape(args.num_envs, num_trainable_agents)\n",
    "                actions[step] = action.reshape(args.num_envs, num_trainable_agents, *envs.single_action_space.shape)\n",
    "                logprobs[step] = logprob.reshape(args.num_envs, num_trainable_agents)\n",
    "\n",
    "            # if step == 0 and iteration%5 == 0:\n",
    "            #     print(\"Sample observation vector for one agent:\")\n",
    "            #     # next_obs has shape (num_envs, num_trainable_agents, obs_dim)\n",
    "            #     print(next_obs[0, 0, :].cpu().numpy()) \n",
    "\n",
    "            # Combine policy actions with random actions\n",
    "            full_actions = np.zeros((args.num_envs, len(possible_agents), *envs.single_action_space.shape), dtype=np.float32)\n",
    "            full_actions[:, trainable_agent_indices] = actions[step].cpu().numpy()\n",
    "            random_actions = np.random.uniform(-1.0, 1.0, size=(args.num_envs, len(random_agent_indices), *envs.single_action_space.shape))\n",
    "            full_actions[:, random_agent_indices] = random_actions\n",
    "\n",
    "            # Environment step\n",
    "            next_obs_all, reward_all, term_all, trunc_all, infos = envs.step(full_actions)\n",
    "            \n",
    "            # Data selection for trainable agents\n",
    "            reward_trainable = reward_all[:, trainable_agent_indices]\n",
    "            next_obs = torch.Tensor(next_obs_all[:, trainable_agent_indices]).to(device)\n",
    "            next_done = torch.Tensor(np.logical_or(term_all[:, trainable_agent_indices], trunc_all[:, trainable_agent_indices])).to(device)\n",
    "            rewards[step] = torch.tensor(reward_trainable).to(device)\n",
    "\n",
    "            game_rewards+=rewards[step]\n",
    "\n",
    "            if np.any(trunc_all):\n",
    "                games+=1\n",
    "                rw+=game_rewards.mean(dim=0)\n",
    "                game_rewards=torch.zeros_like(rewards[0])\n",
    "                any_agent = next(iter(infos[0]))\n",
    "                for ind,info in enumerate(infos):\n",
    "                    score[ind]['blue']+=info[any_agent]['score']['blue']\n",
    "                    score[ind]['red']+=info[any_agent]['score']['red']\n",
    "\n",
    "\n",
    "        blue_avg = sum([s.get('blue',0) for s in score])/args.num_envs\n",
    "        red_avg = sum([s.get('red',0) for s in score])/args.num_envs\n",
    "        #writer.add_scalar(\"charts/blue_score\", blue_avg, global_step)\n",
    "        #writer.add_scalar(\"charts/red_score\", red_avg, global_step)\n",
    "        #rw_sum = rewards.sum(dim=(0, 1))\n",
    "        writer.add_scalar(\"charts/avg_agent0_return\", rw[0]/games, global_step)\n",
    "        writer.add_scalar(\"charts/avg_agent1_return\", rw[1]/games, global_step)\n",
    "\n",
    "        obs_normalizer.update(obs.cpu().numpy().reshape(-1, *envs.single_observation_space.shape))\n",
    "\n",
    "        # --- GAE AND VALUE BOOTSTRAPPING ---\n",
    "        with torch.no_grad():\n",
    "            mean_t = torch.tensor(obs_normalizer.mean, dtype=torch.float32, device=device)\n",
    "            std_t = torch.tensor(obs_normalizer.std, dtype=torch.float32, device=device)\n",
    "\n",
    "            writer.add_scalar(\"stats/running_mean\", mean_t.mean(), global_step)\n",
    "            writer.add_scalar(\"stats/running_std\", std_t.mean(), global_step)\n",
    "\n",
    "            normalized_obs = (obs - mean_t) / \\\n",
    "                     (std_t + 1e-8)\n",
    "            normalized_obs = torch.clamp(normalized_obs, -10, 10)\n",
    "\n",
    "            norm_next_obs = (next_obs - mean_t) / (std_t + 1e-8)\n",
    "            norm_next_obs = torch.clamp(norm_next_obs, -10, 10)\n",
    "            \n",
    "            agent_input = norm_next_obs.reshape(-1, *envs.single_observation_space.shape)\n",
    "            \n",
    "            #next_value = agent.get_value(agent_input).reshape(1, args.num_envs, num_trainable_agents)\n",
    "            next_value = agent.get_value(agent_input).reshape(args.num_envs, num_trainable_agents)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value.squeeze(0)\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues - values[t]\n",
    "                #delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # --- BATCH FLATTENING ---\n",
    "        b_obs = normalized_obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # --- PPO UPDATE PHASE ---\n",
    "        b_inds = np.arange(b_obs.shape[0])\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, b_obs.shape[0], args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                mb_returns = b_returns[mb_inds]\n",
    "                norm_mb_returns = (mb_returns - b_returns.mean()) / (b_returns.std() + 1e-8)\n",
    "\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - norm_mb_returns) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds], -args.clip_coef, args.clip_coef)\n",
    "                    v_loss_clipped = (v_clipped - norm_mb_returns) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - norm_mb_returns) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # --- LOGGING TRAINING LOSSES ---\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        #writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        if args.save_model:\n",
    "            model_path = f\"runs/{run_name}/{args.exp_name}.ppo_model\"\n",
    "            torch.save(agent.state_dict(), model_path)\n",
    "            np.savez(\n",
    "                normalizer_path,\n",
    "                mean=obs_normalizer.mean,\n",
    "                var=obs_normalizer.var\n",
    "            )\n",
    "\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "# import sys\n",
    "\n",
    "# module_directory_path = os.path.abspath('cleanrl')\n",
    "# if module_directory_path not in sys.path:\n",
    "#     sys.path.append(module_directory_path)\n",
    "\n",
    "saved_normalizer = \"runs/run4/latest_normalizer_stats.npz\"\n",
    "model_path = \"runs/run4/ppo_pettingzoo_soccer.ppo_model\"\n",
    "\n",
    "train(config,envs,saved_normalizer=saved_normalizer,model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76adff3a-1138-46e4-9fbb-b577c0586744",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21071f6-0f6a-44a9-a698-d4deeb3a1d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from soccer_env import soccerenv\n",
    "import time\n",
    "# Make sure your Agent class is imported or defined in this file\n",
    "#from your_training_script import Agent, layer_init \n",
    "\n",
    "# --- 1. SETUP AND LOAD THE MODEL ---\n",
    "\n",
    "# The device to run on (e.g., \"cpu\" or \"cuda\")\n",
    "device = torch.device(\"cpu\") \n",
    "\n",
    "# Create a dummy environment to get observation and action space info\n",
    "# This is needed to initialize the Agent class with the correct network shapes\n",
    "dummy_env = soccerenv(render_mode=\"human\")\n",
    "agent = Agent(dummy_env).to(device)\n",
    "dummy_env.close()\n",
    "\n",
    "# Load the saved weights from your training run\n",
    "model_path = f\"runs/run3/{config.exp_name}.ppo_model\" # <--- CHANGE THIS\n",
    "agent.load_state_dict(torch.load(model_path, map_location=device))\n",
    "agent.eval()  # Set the agent to evaluation mode\n",
    "\n",
    "# --- 2. RUN THE EVALUATION LOOP ---\n",
    "\n",
    "env = soccerenv(render_mode=\"human\") # Set \"human\" to visualize, \"None\" to run faster\n",
    "num_episodes = 10\n",
    "\n",
    "trainable_agent_ids = [\"agent_0\", \"agent_1\"]\n",
    "possible_agents = env.possible_agents\n",
    "trainable_agent_indices = [possible_agents.index(a) for a in trainable_agent_ids]\n",
    "red_agent_ids = [a for a in possible_agents if a not in trainable_agent_ids]\n",
    "\n",
    "act_shape = env.action_space(possible_agents[0]).shape\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    obs, infos = env.reset()\n",
    "    final_score = {\"blue\": 0, \"red\": 0}\n",
    "    ep_ret = {agent: 0.0 for agent in env.possible_agents}\n",
    "\n",
    "    while env.agents:\n",
    "        # Build observation batch in the same agent order\n",
    "        obs_tensor = torch.tensor(\n",
    "            np.stack([obs[a] for a in possible_agents]), dtype=torch.float32, device=device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            raw_action = agent.get_deterministic_action(obs_tensor)\n",
    "            norm_action = raw_action.cpu().numpy()  # shape (4,3) if model outputs for all\n",
    "            #print(norm_action)\n",
    "            # If your model only outputs for blue agents, slice accordingly:\n",
    "            # norm_action = torch.tanh(raw_action).cpu().numpy()  # shape (2,3)\n",
    "        \n",
    "        actions = {}\n",
    "        for idx, agent_id in enumerate(possible_agents):\n",
    "            if agent_id in trainable_agent_ids:\n",
    "                # If model outputs only for blue agents, map by index:\n",
    "                src = trainable_agent_indices.index(idx) if norm_action.shape[0] == len(trainable_agent_ids) else idx\n",
    "                actions[agent_id] = norm_action[src].astype(np.float32)\n",
    "            else:\n",
    "                actions[agent_id] = np.random.uniform(-1.0, 1.0, size=act_shape).astype(np.float32)\n",
    "\n",
    "        obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        env.render()\n",
    "        for a, r in rewards.items():\n",
    "            ep_ret[a] += float(r)\n",
    "        env.render()\n",
    "        time.sleep(1/60)\n",
    "\n",
    "        any_agent = next(iter(infos))\n",
    "        if \"score\" in infos[any_agent]:\n",
    "            final_score = infos[any_agent][\"score\"]\n",
    "            \n",
    "    blue_sum = ep_ret.get(\"agent_0\", 0.0) + ep_ret.get(\"agent_1\", 0.0)\n",
    "    #print(f\"Episode {ep}: per-agent={ep_ret}, blue_sum={blue_sum:.4f}\")\n",
    "    #print(f\"Episode {ep+1} final score: blue={final_score.get('blue',0)}, red={final_score.get('red',0)}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d6e8a-8e08-45a2-a5c1-49d4b7002c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl-soccer",
   "language": "python",
   "name": "marl-soccer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
